# Can LLMs crack this binary classification task?

## Table of Contents

- [Introduction](#introduction)
- [Project Description](#project-description)
- [License](#license)
- [Acknowledgments](#acknowledgments)

## Introduction

This project explores the capabilities of Large Language Models (LLMs) in guiding us through a binary classification task. We'll be crafting custom prompts and developing Python modules to seamlessly engage with LLMs within our Jupyter notebook environment. Our focus? Assessing the accuracy, relevance of the responses i.e., code and explanations generated by LLMs. We'll also assess the performance of the classifiers built as a result of running their code on unseen data. Furthermore, we'll compare these LLM-generated solutions with those crafted by the author for the same task. Our ensemble of LLMs includes GPT-3.5, GPT-4, LlaMA-2, and PaLM-2.

## Project Description
The task is to write code to predict defects on a production line and provide relevant explanations when needed. The dataset used for this task was initially curated by Valeo, an automotive supplier company based in France, for the Ecole Normale Sup√©rieure data challenge hosted in 2020. The dataset contains thirteen input features that serve as predictor variables that describe different attributes of the production sample. The target variable indicates whether a given sample exhibits a defect or not. This can be found in the `data` folder of this repository.

The prompt templates are designed around the data understanding, data preparation and modelling phases of the CRISP-DM process. The templates are contained in the `prompt-template` folder. GPT-3.5 and GPT-4 are querying programmatically using the OpenAI API. LlaMA-2 (70B Chat) is also queried programmatically using the Replicate API. However, PaLM-2 is queried through Bard since PaLM API is currently available to only US Residents. For easy interaction with the LLMs in a Jupyter notebook, modules for  querying the different APIs have been created and can be found in the `src` folder.

The actual prompts sent to and responses received from the various LLMs can be found in their respective notebooks in the `experiment/run_2/querying` folder. The execution of the suggested codes  can be found in the `experiment/run_2/implementation` folder. The solution to the binary classification task created by the author can be found in the `human/defect_prediction.ipynb` notebook. 

It is important to note that responses may vary with each run of these files due to the stochastic nature of LLMs.

## License
This project is licensed under the Apache-2.0 License - see the `LICENSE` file for details.

## Acknowledgments
My heartfelt gratitude goes to

* My Project Supervisor, Prof. Daniel Neagu: For his invaluable guidance and feedback that shaped this research.
* OpenAI: For generously providing [access](https://platform.openai.com/docs/guides/gpt) to their Large Language Models (LLMs), GPT-3.5 and GPT-4.
* Replicate: For providing the [platform](https://replicate.com/) to run open-source models like LlaMA-2.
* Datacamp: For providing a tutorial on [using GPT-3.5 and GPT-4 via the OpenAI API in Python](https://www.datacamp.com/tutorial/using-gpt-models-via-the-openai-api-in-python).
* @DataProfessor: For providing examples on [how to query LlaMA-2](https://github.com/dataprofessor/llama2?ref=blog.streamlit.io) using replicate
* To all who engaged in discussions, offered suggestions, and provided encouragement, thank you for being part of this journey.
