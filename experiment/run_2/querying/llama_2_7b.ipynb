{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\Owner\\\\OneDrive\\\\Desktop\\\\MSc. Bradford\\\\MSc. Dissertation\\\\llm_experiment\\\\src')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import llama\n",
    "\n",
    "MODEL = 'a16z-infra/llama-2-7b-chat:d24902e3fa9b698cc208b5e63136c4e26e828659a9f09827ca6ec5bb83014381'\n",
    "SECRET_KEY = os.getenv(\"REPLICATE_API_KEY\")\n",
    "TEMPERATURE = 0.5\n",
    "llama_2 = llama.LLAMA(MODEL,TEMPERATURE, SECRET_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../prompt_templates/data_understanding.txt') as file:\n",
    "    data_understanding_pt = file.read()\n",
    "\n",
    "with open('../../../prompt_templates/data_preparation.txt') as file:\n",
    "    data_preparation_pt = file.read()\n",
    "    \n",
    "with open('../../../prompt_templates/modelling.txt') as file:\n",
    "    modelling_pt = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../../../data/traininginputs.csv')\n",
    "y = pd.read_csv('../../../data/trainingoutput.csv')\n",
    "pred_variables = X.columns[1:].tolist()\n",
    "target_variable = y.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Great! Based on the instruction provided, I will guide you through the data understanding phase of the CRISP-DM process using Python code.\n",
       "Firstly, let's import the necessary libraries:\n",
       "```python\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "from sklearn.preprocessing import MinMaxScaler\n",
       "from sklearn.decomposition import PCA\n",
       "from sklearn.visualization import Plot\n",
       "```\n",
       "Next, we will load the dataset and perform some basic exploratory data analysis (EDA) tasks:\n",
       "```python\n",
       "df = pd.read_csv('data.csv')\n",
       "print(df.head())\n",
       "\n",
       "```\n",
       "This will display the first few rows of the dataset. Let's now visualize the distribution of the predictor variables:\n",
       "\n",
       "```python\n",
       "plt = df[['OP070_V_1_angle_value', 'OP090_SnapRingPeakForce_value', 'OP070_V_2_angle_value', 'OP120_Rodage_I_mesure_value', 'OP090_SnapRingFinalStroke_value', 'OP110_Vissage_M8_torque_value', 'OP100_Capuchon_insertion_mesure', 'OP120_Rodage_U_mesure_value', 'OP070_V_1_torque_value', 'OP090_StartLinePeakForce_value', 'OP110_Vissage_M8_angle_value', 'OP090_SnapRingMidPointForce_val']].describe()\n",
       "print(plt)\n",
       "```\n",
       "This will show us the mean, standard deviation, and count of each variable in the dataset. From this, we can see that some variables have a skewed distribution, while others are more normally distributed. We also observe that there are missing values for some variables.\n",
       "Now, let's perform principal component analysis (PCA) to reduce the dimensionality of the data:\n",
       "```python\n",
       "from sklearn.decomposition import PCA\n",
       "pca = PCA(n_components=2)\n",
       "X = scaled_data_train[:, :2]  # only use first two components\n",
       "y = pca.fit_transform(scaled_data_train)\n",
       "\n",
       "```\n",
       "Here, `scaled_data_train` is the trained dataset after scaling the predictor variables using Min-Max Scaler. The `n_components` parameter specifies the number of dimensions to retain in the reduced space. In this case, we want to keep only two dimensions, which correspond to the most important features in the dataset.\n",
       "Finally, let's visualize the results using a scatter plot:\n",
       "\n",
       "```python\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "plt = pd.melt(y, id_vars='target', value_vars=X, var_name='Predictors')\n",
       "\n",
       "plt['Target'] = binarize(t['Target'], threshold=0.5)\n",
       "\n",
       "plt.plot(kind='scatter', x='Predictors', y='Target')\n",
       "plt.set_xlabel('Predictors')\n",
       "plt.set_ylabel('Binarized Target')\n",
       "plt.title('Relationship between Predictors and Binarized Target')\n",
       "\n",
       "plt.show()\n",
       "\n",
       "```\n",
       "This will create a scatter plot showing the relationship between the predictor variables and the binary target variable. From this plot, we can see if there are any obvious correlations or patterns between the predictor variables and the target variable.\n",
       "That's it for now! We have completed the data understanding phase of the CRISP-DM process by performing EDA tasks and reducing the dimensionality of the data using PCA. The next step would be to train a machine learning model on the preprocessed data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = data_understanding_pt.format(pred_variables, target_variable, None)\n",
    "llama_2.query_llama(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
