{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\Owner\\\\OneDrive\\\\Desktop\\\\MSc. Bradford\\\\MSc. Dissertation\\\\llm_experiment\\\\src')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import gpt\n",
    "\n",
    "MODEL = 'gpt-3.5-turbo-0613'\n",
    "SECRET_KEY = os.getenv(\"OPENAI_SECRET_KEY\")\n",
    "TEMPERATURE = 0.5\n",
    "gpt_3_5 = gpt.GPT(MODEL,TEMPERATURE, SECRET_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../prompt_templates/data_understanding.txt') as file:\n",
    "    data_understanding_pt = file.read()\n",
    "\n",
    "with open('../../../prompt_templates/data_preparation.txt') as file:\n",
    "    data_preparation_pt = file.read()\n",
    "\n",
    "with open('../../../prompt_templates/modelling.txt') as file:\n",
    "    modelling_pt = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../../../data/traininginputs.csv')\n",
    "y = pd.read_csv('../../../data/trainingoutput.csv')\n",
    "pred_variables = X.columns[1:].tolist()\n",
    "target_variable = y.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To start the data understanding phase, we will first import the necessary libraries: pandas, numpy, and matplotlib. Pandas will be used for data manipulation and analysis, numpy for numerical operations, and matplotlib for data visualization.\n",
       "\n",
       "Next, we will load the data into a pandas DataFrame and inspect the first few rows to get an initial understanding of the data structure.\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "# Load the data into a pandas DataFrame\n",
       "data = pd.read_csv('data.csv')\n",
       "\n",
       "# Inspect the first few rows of the data\n",
       "print(data.head())\n",
       "```\n",
       "\n",
       "After loading the data, we can generate some statistical summaries to gain insights into the data distribution and identify any potential issues.\n",
       "\n",
       "```python\n",
       "# Generate statistical summaries\n",
       "print(data.describe())\n",
       "```\n",
       "\n",
       "This will provide summary statistics such as count, mean, standard deviation, minimum, and maximum values for each numerical variable. It will help us understand the range and distribution of the data.\n",
       "\n",
       "Next, we can visualize the data using histograms, box plots, and scatter plots to further explore the relationships between variables and identify any outliers or patterns.\n",
       "\n",
       "```python\n",
       "# Visualize the data using histograms\n",
       "data.hist(figsize=(10, 8))\n",
       "plt.tight_layout()\n",
       "plt.show()\n",
       "\n",
       "# Visualize the data using box plots\n",
       "data.boxplot(figsize=(10, 8))\n",
       "plt.tight_layout()\n",
       "plt.show()\n",
       "\n",
       "# Visualize the relationships between variables using scatter plots\n",
       "pd.plotting.scatter_matrix(data, figsize=(12, 12))\n",
       "plt.tight_layout()\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "The histograms will give us an overview of the distribution of each variable, while the box plots will help identify any outliers. The scatter plots will show the relationships between pairs of variables, which can be useful for identifying correlations.\n",
       "\n",
       "For the data preparation phase, some important things to note are:\n",
       "\n",
       "1. Missing Values: Check for missing values in the dataset and decide on an appropriate strategy for handling them. This could involve imputing missing values or removing rows/columns with missing values.\n",
       "\n",
       "2. Categorical Variables: Identify if there are any categorical variables in the dataset and consider encoding them appropriately for machine learning algorithms.\n",
       "\n",
       "3. Feature Scaling: Depending on the machine learning algorithms to be used, it may be necessary to scale the predictor variables to a similar range to avoid biasing the model towards variables with larger magnitudes.\n",
       "\n",
       "4. Feature Selection: Evaluate the relevance and importance of predictor variables in relation to the target variable. Consider performing feature selection techniques to select the most informative variables for the model.\n",
       "\n",
       "5. Data Split: Split the dataset into training and testing sets to evaluate the model's performance on unseen data. This can be done using techniques such as cross-validation or a simple train-test split.\n",
       "\n",
       "By following these steps, we can gain a better understanding of the data and prepare it appropriately for building a machine learning model."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = data_understanding_pt.format(pred_variables, target_variable, None)\n",
    "gpt_3_5.query_gpt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_report = ['There are outliers present as observed from the boxplot',\n",
    "                    'The OP100_Capuchon_insertion_mesure feature has some missing values']\n",
    "exploration_report = ['Some features have a skewed distribution',\n",
    "                        'The target variable has an imbalanced class distribution',\n",
    "                        'There is little or no correlation between the predictor and target variables',\n",
    "                        'Some predictor variables are correlated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To prepare the data for modeling, we need to address the issues mentioned in the data quality report and data exploration report. Here's how we can handle each issue:\n",
       "\n",
       "1. Outliers: Since outliers are present in the data, we can consider using a robust scaler, such as the RobustScaler from the scikit-learn library, to scale the predictor variables. This scaler is less sensitive to outliers and can help mitigate their impact on the model.\n",
       "\n",
       "2. Missing Values: The feature 'OP100_Capuchon_insertion_mesure' has missing values. We can handle this by either imputing the missing values or removing the rows/columns with missing values. To impute missing values, we can use techniques such as mean imputation or regression imputation from libraries like scikit-learn or pandas.\n",
       "\n",
       "3. Skewed Distribution: As some features have a skewed distribution, we can apply a transformation to make the distribution more symmetric. One common transformation is the logarithmic transformation, which can be applied using the numpy library's log function.\n",
       "\n",
       "4. Imbalanced Class Distribution: Since the target variable has an imbalanced class distribution, we need to address this issue to prevent the model from being biased towards the majority class. Techniques such as oversampling the minority class (e.g., using the SMOTE algorithm from the imbalanced-learn library) or undersampling the majority class can be applied to balance the classes.\n",
       "\n",
       "5. Correlation: Although there is little or no correlation between the predictor and target variables, some predictor variables are correlated. This can cause multicollinearity issues in the model. It is important to address this by either removing one of the correlated variables or using techniques like principal component analysis (PCA) to reduce the dimensionality of the dataset.\n",
       "\n",
       "Here's an example of how the data preparation phase could be implemented:\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "from sklearn.preprocessing import RobustScaler\n",
       "from sklearn.impute import SimpleImputer\n",
       "from imblearn.over_sampling import SMOTE\n",
       "\n",
       "# Load the data into a pandas DataFrame\n",
       "data = pd.read_csv('data.csv')\n",
       "\n",
       "# Handling missing values\n",
       "imputer = SimpleImputer(strategy='mean')\n",
       "data['OP100_Capuchon_insertion_mesure'] = imputer.fit_transform(data['OP100_Capuchon_insertion_mesure'].values.reshape(-1, 1))\n",
       "\n",
       "# Handling outliers\n",
       "scaler = RobustScaler()\n",
       "data[predictor_variables] = scaler.fit_transform(data[predictor_variables])\n",
       "\n",
       "# Handling skewed distribution\n",
       "data[predictor_variables] = np.log1p(data[predictor_variables])\n",
       "\n",
       "# Handling imbalanced class distribution\n",
       "X = data[predictor_variables]\n",
       "y = data['OP130_Resultat_Global_v']\n",
       "smote = SMOTE()\n",
       "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
       "\n",
       "# Summary of important things to note for the modeling phase:\n",
       "# - Choose appropriate evaluation metrics for imbalanced classification, such as precision, recall, F1-score, or area under the ROC curve.\n",
       "# - Consider using ensemble methods, such as random forests or gradient boosting, which can handle imbalanced datasets better.\n",
       "# - Perform feature selection to select the most informative variables for the model.\n",
       "# - Split the data into training and testing sets for model evaluation.\n",
       "# - Apply appropriate cross-validation techniques to assess the model's performance on unseen data.\n",
       "```\n",
       "\n",
       "In the modeling phase, it is important to choose appropriate evaluation metrics for imbalanced classification and consider using ensemble methods that can handle imbalanced datasets better. Feature selection, data splitting, and cross-validation techniques should also be applied."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = data_preparation_pt.format(pred_variables, target_variable, quality_report, exploration_report, None)\n",
    "gpt_3_5.query_gpt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = ['f1-score', 'roc_auc']\n",
    "notes = ['The target variable has an imbalanced class distribution', \n",
    "        'The data has not yet been split into train and test sets',\n",
    "        'The code for balancing the data with SMOTE failed because of the presence of missing values after scaling was performed.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To perform the modeling phase, we will first import the necessary libraries: pandas, numpy, and scikit-learn. These libraries provide various machine learning algorithms, evaluation metrics, and tools for model training and evaluation.\n",
       "\n",
       "Next, we will load the prepared dataset and split it into training and testing sets. It is important to split the data to evaluate the model's performance on unseen data. We will use the train_test_split function from scikit-learn for this purpose.\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "from sklearn.model_selection import train_test_split\n",
       "\n",
       "# Load the prepared dataset\n",
       "data = pd.read_csv('prepared_data.csv')\n",
       "\n",
       "# Split the data into training and testing sets\n",
       "X = data[predictor_variables]\n",
       "y = data['OP130_Resultat_Global_v']\n",
       "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
       "```\n",
       "\n",
       "Since the target variable has an imbalanced class distribution, we need to address this issue to prevent the model from being biased towards the majority class. However, the code for balancing the data with SMOTE failed because of the presence of missing values after scaling was performed. In this case, we can proceed with training the models on the imbalanced dataset and evaluate their performance using appropriate evaluation metrics.\n",
       "\n",
       "We will train five different models based on different algorithms and evaluate their performance using the f1-score and roc_auc metrics. The f1-score is a measure of the model's accuracy, combining precision and recall, while the roc_auc metric measures the model's ability to discriminate between the positive and negative classes.\n",
       "\n",
       "Here's an example of how the modeling phase could be implemented:\n",
       "\n",
       "```python\n",
       "from sklearn.linear_model import LogisticRegression\n",
       "from sklearn.tree import DecisionTreeClassifier\n",
       "from sklearn.ensemble import RandomForestClassifier\n",
       "from sklearn.svm import SVC\n",
       "from sklearn.metrics import f1_score, roc_auc_score\n",
       "\n",
       "# Train and evaluate logistic regression model\n",
       "logreg = LogisticRegression()\n",
       "logreg.fit(X_train, y_train)\n",
       "logreg_pred = logreg.predict(X_test)\n",
       "logreg_f1 = f1_score(y_test, logreg_pred)\n",
       "logreg_roc_auc = roc_auc_score(y_test, logreg_pred)\n",
       "\n",
       "# Train and evaluate decision tree model\n",
       "dt = DecisionTreeClassifier()\n",
       "dt.fit(X_train, y_train)\n",
       "dt_pred = dt.predict(X_test)\n",
       "dt_f1 = f1_score(y_test, dt_pred)\n",
       "dt_roc_auc = roc_auc_score(y_test, dt_pred)\n",
       "\n",
       "# Train and evaluate random forest model\n",
       "rf = RandomForestClassifier()\n",
       "rf.fit(X_train, y_train)\n",
       "rf_pred = rf.predict(X_test)\n",
       "rf_f1 = f1_score(y_test, rf_pred)\n",
       "rf_roc_auc = roc_auc_score(y_test, rf_pred)\n",
       "\n",
       "# Train and evaluate support vector machine model\n",
       "svm = SVC()\n",
       "svm.fit(X_train, y_train)\n",
       "svm_pred = svm.predict(X_test)\n",
       "svm_f1 = f1_score(y_test, svm_pred)\n",
       "svm_roc_auc = roc_auc_score(y_test, svm_pred)\n",
       "\n",
       "# Create a summary of the model performances\n",
       "model_summary = pd.DataFrame({\n",
       "    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest', 'Support Vector Machine'],\n",
       "    'F1-Score': [logreg_f1, dt_f1, rf_f1, svm_f1],\n",
       "    'ROC AUC': [logreg_roc_auc, dt_roc_auc, rf_roc_auc, svm_roc_auc]\n",
       "})\n",
       "\n",
       "print(model_summary)\n",
       "```\n",
       "\n",
       "In this example, we trained and evaluated four different models: logistic regression, decision tree, random forest, and support vector machine. We calculated the f1-score and roc_auc for each model and created a summary table to compare their performances.\n",
       "\n",
       "The choice of these algorithms was based on their suitability for binary classification tasks and their availability in scikit-learn. Logistic regression is a simple and interpretable algorithm, decision tree and random forest are non-linear algorithms that can capture complex relationships, and support vector machine is a powerful algorithm for separating classes in high-dimensional space.\n",
       "\n",
       "By evaluating multiple models and comparing their performances using appropriate evaluation metrics, we can determine the best modeling approach for predicting defects on the production line."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = modelling_pt.format(pred_variables, target_variable, eval_metrics, notes)\n",
    "gpt_3_5.query_gpt(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
