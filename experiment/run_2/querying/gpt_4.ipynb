{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\Owner\\\\OneDrive\\\\Desktop\\\\MSc. Bradford\\\\MSc. Dissertation\\\\llm_experiment\\\\src')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import gpt\n",
    "\n",
    "MODEL = 'gpt-4-0613'\n",
    "SECRET_KEY = os.getenv(\"OPENAI_SECRET_KEY\")\n",
    "TEMPERATURE = 0.5\n",
    "gpt_4 = gpt.GPT(MODEL,TEMPERATURE, SECRET_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../prompt_templates/data_understanding.txt') as file:\n",
    "    data_understanding_pt = file.read()\n",
    "\n",
    "with open('../../../prompt_templates/data_preparation.txt') as file:\n",
    "    data_preparation_pt = file.read()\n",
    "    \n",
    "with open('../../../prompt_templates/modelling.txt') as file:\n",
    "    modelling_pt = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../../../data/traininginputs.csv')\n",
    "y = pd.read_csv('../../../data/trainingoutput.csv')\n",
    "pred_variables = X.columns[1:].tolist()\n",
    "target_variable = y.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To execute the data understanding phase of the CRISP-DM process for the given data, we need to use Python libraries such as pandas for data manipulation, matplotlib and seaborn for data visualization, and sklearn for machine learning tasks.\n",
       "\n",
       "Here is a sample Python code that can be used for this purpose:\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "from sklearn.preprocessing import StandardScaler\n",
       "\n",
       "# Assuming data is stored in a CSV file\n",
       "df = pd.read_csv('data.csv')\n",
       "\n",
       "# Display the first few rows of the dataframe\n",
       "print(df.head())\n",
       "\n",
       "# Display the summary statistics of the dataframe\n",
       "print(df.describe())\n",
       "\n",
       "# Check if there are any missing values in the dataframe\n",
       "print(df.isnull().sum())\n",
       "\n",
       "# Plot histograms for each predictor variable\n",
       "df.hist(bins=50, figsize=(20,15))\n",
       "plt.show()\n",
       "\n",
       "# Plot a correlation matrix to understand the relationship between variables\n",
       "corr_matrix = df.corr()\n",
       "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
       "\n",
       "# Standardize the predictor variables\n",
       "scaler = StandardScaler()\n",
       "df[df.columns[:-1]] = scaler.fit_transform(df[df.columns[:-1]])\n",
       "\n",
       "# Display the first few rows of the dataframe after standardization\n",
       "print(df.head())\n",
       "```\n",
       "\n",
       "Explanation for the choice of methods and Python libraries:\n",
       "- Pandas is used for data manipulation and analysis. It is used here to load the data from a CSV file into a dataframe, which is a 2-dimensional labeled data structure with columns of potentially different types.\n",
       "- Matplotlib and seaborn are used for data visualization. Histograms are plotted for each predictor variable to understand their distributions. A heatmap of the correlation matrix is plotted to understand the relationship between variables.\n",
       "- Sklearn's StandardScaler is used to standardize the predictor variables by removing the mean and scaling to unit variance.\n",
       "\n",
       "Summary of important things to note for the data preparation phase:\n",
       "- Missing values: If there are any missing values in the dataframe, we will need to handle them in the data preparation phase. This can be done by either removing the rows with missing values or filling the missing values with the mean, median, or mode of the respective columns.\n",
       "- Outliers: If there are any outliers in the predictor variables, we might need to handle them as they can affect the performance of the machine learning model. This can be done using methods such as the IQR score or Z-score.\n",
       "- Standardization: As we have standardized the predictor variables in the data understanding phase, we need to remember to apply the same transformation to any new data in the future before making predictions using the machine learning model.\n",
       "- Correlation: If two predictor variables are highly correlated, we might need to consider removing one of them to avoid multicollinearity."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = data_understanding_pt.format(pred_variables, target_variable, None)\n",
    "gpt_4.query_gpt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_report = ['There might be outliers present since some of the distribution is skewed',\n",
    "                    'The OP100_Capuchon_insertion_mesure feature has some missing values']\n",
    "exploration_report = ['Some features have a skewed distribution',\n",
    "                        'The target variable has an imbalanced class distribution',\n",
    "                        'There is weak correlation between the predictor and target variables',\n",
    "                        'There is notable correlation between some predictor variables']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Given the data quality and exploration reports, we need to handle outliers, missing values, skewed distribution, imbalanced class distribution, and correlation among predictor variables. We will use Python libraries such as pandas for data manipulation, numpy for numerical operations, sklearn for machine learning tasks, and imbalanced-learn to handle imbalanced datasets.\n",
       "\n",
       "Here is a sample Python code that can be used for this purpose:\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
       "from sklearn.impute import SimpleImputer\n",
       "from sklearn.feature_selection import VarianceThreshold\n",
       "from imblearn.over_sampling import SMOTE\n",
       "\n",
       "# Assuming data is stored in a CSV file\n",
       "df = pd.read_csv('data.csv')\n",
       "\n",
       "# Handle missing values\n",
       "imputer = SimpleImputer(strategy='median')\n",
       "df['OP100_Capuchon_insertion_mesure'] = imputer.fit_transform(df['OP100_Capuchon_insertion_mesure'].values.reshape(-1,1))\n",
       "\n",
       "# Handle outliers\n",
       "Q1 = df.quantile(0.25)\n",
       "Q3 = df.quantile(0.75)\n",
       "IQR = Q3 - Q1\n",
       "df = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
       "\n",
       "# Handle skewed distribution\n",
       "pt = PowerTransformer(method='yeo-johnson', standardize=True)\n",
       "df[df.columns] = pt.fit_transform(df[df.columns])\n",
       "\n",
       "# Handle correlation among predictor variables\n",
       "correlated_features = set()\n",
       "correlation_matrix = df.corr()\n",
       "for i in range(len(correlation_matrix.columns)):\n",
       "    for j in range(i):\n",
       "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
       "            colname = correlation_matrix.columns[i]\n",
       "            correlated_features.add(colname)\n",
       "df.drop(labels=correlated_features, axis=1, inplace=True)\n",
       "\n",
       "# Handle imbalanced class distribution\n",
       "smote = SMOTE(random_state=42)\n",
       "X, y = smote.fit_resample(df[df.columns[:-1]], df[df.columns[-1]])\n",
       "\n",
       "# Display the first few rows of the dataframe after preprocessing\n",
       "print(df.head())\n",
       "```\n",
       "\n",
       "Explanation for the choice of methods and Python libraries:\n",
       "- SimpleImputer is used to handle missing values. The strategy used here is 'median', which means that missing values are filled with the median of the respective columns.\n",
       "- The Interquartile Range (IQR) method is used to handle outliers. Rows that have values outside the range (Q1 - 1.5 * IQR, Q3 + 1.5 * IQR) are removed.\n",
       "- PowerTransformer is used to handle skewed distribution. The method used here is 'yeo-johnson', which can handle both positive and negative values.\n",
       "- A correlation matrix is used to handle correlation among predictor variables. If the absolute correlation between two predictor variables is more than 0.8, one of them is removed.\n",
       "- SMOTE (Synthetic Minority Over-sampling Technique) is used to handle imbalanced class distribution. It works by creating synthetic samples from the minor class.\n",
       "\n",
       "Summary of important things to note for the modelling phase:\n",
       "- The same preprocessing steps applied to the training data should also be applied to any new data in the future before making predictions using the machine learning model.\n",
       "- The target variable has been balanced using SMOTE, which might increase the likelihood of overfitting. Therefore, it might be necessary to use techniques such as cross-validation to ensure that the model generalizes well to unseen data.\n",
       "- Some predictor variables have been removed due to high correlation. Therefore, the model might not take into account the effect of these variables. If the performance of the model is not satisfactory, it might be worth considering to include these variables again and use a model that can handle multicollinearity."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = data_preparation_pt.format(pred_variables, target_variable, quality_report, exploration_report, None)\n",
    "gpt_4.query_gpt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = ['f1-score', 'roc_auc']\n",
    "notes = ['The target variable has an imbalanced class distribution', \n",
    "        'The data has not yet been split into train and test sets',\n",
    "        'Preprocessing such as handling missing values, outliers, skewed distributions and multicollinearity was performed on the entire dataset',\n",
    "        'The target variable has been balanced using SMOTE, which might increase the likelihood of overfitting. Therefore, it might be necessary to use techniques such as cross-validation to ensure that the model generalizes well to unseen data.',\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Given the context, we need to build a binary classifier to predict the defect on the production line. We will use Python libraries such as pandas for data manipulation, sklearn for machine learning tasks, and matplotlib for data visualization.\n",
       "\n",
       "Here is a sample Python code that can be used for this purpose:\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "from sklearn.model_selection import train_test_split, cross_val_score\n",
       "from sklearn.metrics import f1_score, roc_auc_score\n",
       "from sklearn.linear_model import LogisticRegression\n",
       "from sklearn.tree import DecisionTreeClassifier\n",
       "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
       "from sklearn.svm import SVC\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "# Assuming data is stored in a CSV file\n",
       "df = pd.read_csv('data.csv')\n",
       "\n",
       "# Split the data into train and test sets\n",
       "X = df[df.columns[:-1]]\n",
       "y = df[df.columns[-1]]\n",
       "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
       "\n",
       "# Define the models\n",
       "models = [\n",
       "    LogisticRegression(),\n",
       "    DecisionTreeClassifier(),\n",
       "    RandomForestClassifier(),\n",
       "    GradientBoostingClassifier(),\n",
       "    SVC(probability=True)\n",
       "]\n",
       "\n",
       "# Train and evaluate the models\n",
       "results = {}\n",
       "for model in models:\n",
       "    model_name = model.__class__.__name__\n",
       "    model.fit(X_train, y_train)\n",
       "    y_pred = model.predict(X_test)\n",
       "    f1 = f1_score(y_test, y_pred)\n",
       "    roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
       "    results[model_name] = [f1, roc_auc]\n",
       "\n",
       "# Plot the results\n",
       "df_results = pd.DataFrame(results, index=['f1-score', 'roc_auc']).transpose()\n",
       "df_results.plot(kind='bar', ylim=(0,1))\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "Explanation for the choice of methods, algorithms, and Python libraries:\n",
       "- train_test_split is used to split the data into train and test sets. This is necessary to evaluate the performance of the models on unseen data.\n",
       "- We use five different algorithms to train the models: Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, and Support Vector Machine. These algorithms were chosen because they are commonly used for binary classification tasks and they have different strengths and weaknesses, which makes it interesting to compare their performance.\n",
       "- f1_score and roc_auc_score are used as evaluation metrics. The F1 score is the harmonic mean of precision and recall, and it is a good metric to use when the classes are imbalanced. The ROC AUC score is the area under the receiver operating characteristic curve, and it measures the ability of the model to distinguish between the classes.\n",
       "- We plot the results using a bar chart to visualize the performance of the models.\n",
       "\n",
       "Summary of important things to note for the modelling phase:\n",
       "- The same preprocessing steps applied to the training data should also be applied to any new data in the future before making predictions using the machine learning model.\n",
       "- The target variable has been balanced using SMOTE, which might increase the likelihood of overfitting. Therefore, we use cross-validation to ensure that the model generalizes well to unseen data.\n",
       "- The performance of the models might be affected by the imbalanced class distribution of the target variable, the handling of missing values, outliers, skewed distributions, and multicollinearity in the data preparation phase.\n",
       "- The models might need to be fine-tuned to achieve better performance. This can be done using methods such as grid search or random search."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = modelling_pt.format(pred_variables, target_variable, eval_metrics, notes)\n",
    "gpt_4.query_gpt(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
