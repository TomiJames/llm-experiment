{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gpt import GPT\n",
    "\n",
    "MODEL = 'gpt-3.5-turbo-0613'\n",
    "SECRET_KEY = os.getenv(\"GPT_SECRET_KEY\")\n",
    "TEMPERATURE = 0.7\n",
    "gpt_3_5 = GPT(MODEL,TEMPERATURE, SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valeo = pd.read_csv('valeo_dataset.csv')\n",
    "n_rows, n_cols = valeo.shape\n",
    "cols = valeo.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The CRISP-DM (Cross-Industry Standard Process for Data Mining) process is a widely used framework for developing data mining and analytics projects. Here are the steps needed to develop an end-to-end project using the CRISP-DM process:\n",
       "\n",
       "1. Business Understanding:\n",
       "   - Clearly define the problem statement and the business objective.\n",
       "   - Understand the requirements and expectations from the project.\n",
       "   - Determine the success criteria and how the classification problem will contribute to achieving those objectives.\n",
       "\n",
       "2. Data Understanding:\n",
       "   - Gather and explore the Valeo dataset.\n",
       "   - Identify the types of variables and their meaning.\n",
       "   - Check for missing values, outliers, and data quality issues.\n",
       "   - Understand the relationship between the predictor variables and the target variable.\n",
       "   - Identify any patterns, trends, or insights in the data.\n",
       "\n",
       "3. Data Preparation:\n",
       "   - Clean the dataset by addressing missing values, outliers, and data quality issues.\n",
       "   - Handle categorical variables by encoding or transforming them into numerical representations.\n",
       "   - Split the dataset into training and testing sets.\n",
       "   - Perform any necessary feature scaling or normalization.\n",
       "\n",
       "4. Modeling:\n",
       "   - Select an appropriate binary classification algorithm (e.g., logistic regression, decision trees, random forests, support vector machines).\n",
       "   - Train the selected model using the training dataset.\n",
       "   - Evaluate the model's performance using appropriate evaluation metrics (e.g., accuracy, precision, recall, F1 score, ROC AUC).\n",
       "\n",
       "5. Evaluation:\n",
       "   - Assess the model's performance on the testing dataset.\n",
       "   - Compare the model's performance against the success criteria defined in the business understanding phase.\n",
       "   - Determine if the model meets the requirements and if further improvements are needed.\n",
       "\n",
       "6. Deployment:\n",
       "   - Once the model is deemed satisfactory, deploy it to production.\n",
       "   - Integrate the model into the existing system or application.\n",
       "   - Ensure the model is used effectively and monitored for performance.\n",
       "\n",
       "7. Maintenance:\n",
       "   - Continuously monitor the model's performance and retrain it if necessary.\n",
       "   - Update the model as new data becomes available or as business requirements change.\n",
       "   - Regularly review the model's impact on the business objective and adjust as needed.\n",
       "\n",
       "It is important to note that these steps are iterative and may require going back and forth between different stages to refine the model and improve its performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "You are assisting a student who is new to data science and big data analytics. The student intends to predict defects on a production line using the valeo dataset.\n",
    "The dataset contains {n_rows} rows and {n_cols} columns. The value to be classified is in the {cols[-1]} column. The values used to predict the value are in the {cols[:-1]} columns.\n",
    "Using the CRISP-DM process, list the steps needed to develop an end-to-end project that solves this problem.\n",
    "\"\"\"\n",
    "\n",
    "gpt_3_5.query_gpt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Here's a Python code snippet to carry out the data understanding phase for the given Valeo dataset:\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "\n",
       "# Load the dataset\n",
       "df = pd.read_csv('valeo_dataset.csv')\n",
       "\n",
       "# Display the shape of the dataset\n",
       "print(\"Dataset shape:\", df.shape)\n",
       "\n",
       "# Display the column names\n",
       "print(\"Column names:\", df.columns)\n",
       "\n",
       "# Display the first few rows of the dataset\n",
       "print(\"First few rows:\\n\", df.head())\n",
       "\n",
       "# Check for missing values\n",
       "print(\"Missing values:\\n\", df.isnull().sum())\n",
       "\n",
       "# Check data types of variables\n",
       "print(\"Data types:\\n\", df.dtypes)\n",
       "\n",
       "# Check basic statistics of the dataset\n",
       "print(\"Summary statistics:\\n\", df.describe())\n",
       "\n",
       "# Check the unique values in the target variable\n",
       "print(\"Unique values in the target variable:\\n\", df['Binar_OP130_Resultat_Global_v'].unique())\n",
       "\n",
       "# Explore the relationship between predictor variables and the target variable\n",
       "print(\"Correlation with the target variable:\\n\", df.corr()['Binar_OP130_Resultat_Global_v'])\n",
       "\n",
       "# Visualize the distribution of the target variable\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "plt.hist(df['Binar_OP130_Resultat_Global_v'])\n",
       "plt.xlabel(\"Target variable values\")\n",
       "plt.ylabel(\"Frequency\")\n",
       "plt.title(\"Distribution of the target variable\")\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "Make sure to replace `'valeo_dataset.csv'` with the actual filename and path of the Valeo dataset on your local system.\n",
       "\n",
       "This code will load the dataset, display its shape and column names, show the first few rows, check for missing values, check data types, provide summary statistics, display unique values in the target variable, explore the correlation between predictor variables and the target variable, and visualize the distribution of the target variable.\n",
       "\n",
       "You can run this code in a Jupyter Notebook or any Python environment to understand the data better."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "The business objective is to predict a defect on the production line captured in this dataset. Write a python code to carry out the data understanding phase as outlined in the project plan.\n",
    "\"\"\"\n",
    "\n",
    "gpt_3_5.query_gpt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Thank you for providing feedback on the initial code. Let's address the issues you observed and take steps to better understand the data:\n",
       "\n",
       "1. Missing values:\n",
       "   - Identify the feature with over 50% missing values and determine if it is still valuable for analysis. If not, consider dropping the feature from the dataset.\n",
       "   - If the feature is important, explore potential methods to handle missing values such as imputation techniques (mean, median, mode), using a separate category for missing values, or advanced imputation methods like k-Nearest Neighbors (KNN) imputation.\n",
       "\n",
       "2. Imbalanced target variable:\n",
       "   - Consider the impact of the imbalanced target variable on the modeling process. Imbalanced classes can lead to biased models that favor the majority class.\n",
       "   - Explore techniques to address class imbalance, such as oversampling the minority class (e.g., Synthetic Minority Over-sampling Technique or SMOTE) or undersampling the majority class.\n",
       "   - Alternatively, utilize algorithms that handle class imbalance inherently, like weighted loss functions or ensemble methods like Random Forests or Gradient Boosting.\n",
       "\n",
       "3. Outliers and data quality issues:\n",
       "   - Explore the dataset for outliers by examining box plots, scatter plots, or using statistical methods like the Z-score or IQR (Interquartile Range).\n",
       "   - Decide whether to remove outliers or transform them (e.g., winsorization, log transformation) based on the domain knowledge and impact on the analysis.\n",
       "   - Address data quality issues by cleaning the dataset, removing or correcting erroneous values, and ensuring consistency.\n",
       "\n",
       "4. Correlation between predictor variables and the target variable:\n",
       "   - If the initial correlation analysis did not show significant correlations, consider exploring non-linear relationships or interactions between variables.\n",
       "   - Use different correlation measures like Spearman's rank correlation coefficient for non-linear relationships.\n",
       "   - Visualize relationships between predictor variables and the target variable using scatter plots, box plots, or other suitable visualization techniques.\n",
       "\n",
       "After addressing these issues and gaining a better understanding of the data, you can proceed with feature engineering, modeling, and evaluation stages. Remember that data understanding is an iterative process, and it is essential to continually refine and update your understanding as you progress through the project."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\" \n",
    "After executing the codes you shared earlier. I observed the following:\n",
    "1. One of the features has over 50% missing values.\n",
    "2. The target variable is highly imbalanced.\n",
    "3. None of the codes addressed the outliers or data quality issues.\n",
    "4. None of the predictor variables seem to be correlated with the target variable using the df.corr() method.\n",
    "\n",
    "How do I address these issues to better understand the data and proceed with analysis?\n",
    "\"\"\"\n",
    "\n",
    "gpt_3_5.query_gpt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Here's a Python code snippet to address the first, third, and fourth issues you mentioned:\n",
       "\n",
       "1. Handling missing values:\n",
       "```python\n",
       "# Identify features with over 50% missing values\n",
       "missing_values = df.isnull().sum() / len(df) * 100\n",
       "features_with_missing_values = missing_values[missing_values > 50].index\n",
       "\n",
       "# Drop features with over 50% missing values\n",
       "df.drop(columns=features_with_missing_values, inplace=True)\n",
       "```\n",
       "\n",
       "2. Handling outliers and data quality issues:\n",
       "```python\n",
       "# Identify outliers using Z-score\n",
       "from scipy.stats import zscore\n",
       "\n",
       "z_scores = zscore(df)\n",
       "outlier_indices = (abs(z_scores) > 3).any(axis=1)\n",
       "\n",
       "# Remove outliers from the dataset\n",
       "df = df[~outlier_indices]\n",
       "```\n",
       "\n",
       "3. Exploring correlations between predictor variables and the target variable:\n",
       "```python\n",
       "# Calculate correlations using Spearman's rank correlation coefficient\n",
       "correlations = df.corr(method='spearman')['Binar_OP130_Resultat_Global_v']\n",
       "\n",
       "# Filter variables with significant correlation\n",
       "significant_correlations = correlations[abs(correlations) > 0.1].index\n",
       "\n",
       "# Visualize relationships between significant variables and the target variable\n",
       "import seaborn as sns\n",
       "\n",
       "sns.pairplot(data=df, x_vars=significant_correlations, y_vars='Binar_OP130_Resultat_Global_v')\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "Make sure to include these code snippets after the initial data understanding code provided earlier.\n",
       "\n",
       "In the first code snippet, we identify features with over 50% missing values and drop them from the dataset.\n",
       "\n",
       "In the second code snippet, we use the Z-score method to identify outliers and remove them from the dataset.\n",
       "\n",
       "In the third code snippet, we calculate correlations using Spearman's rank correlation coefficient, filter variables with significant correlations, and visualize the relationships between the significant variables and the target variable using a pair plot.\n",
       "\n",
       "Remember to adjust the thresholds and methods used based on your specific dataset and requirements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\" \n",
    "Write a python code to implement the suggestions provided in the first, third and fourth issues.\n",
    "\"\"\"\n",
    "\n",
    "gpt_3_5.query_gpt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To resolve the FutureWarning message, you can explicitly set the `numeric_only` parameter to `True` in the `corr()` function call. Here's an updated code snippet to address the issue:\n",
       "\n",
       "```python\n",
       "# Calculate correlations using Spearman's rank correlation coefficient\n",
       "correlations = df.corr(method='spearman', numeric_only=True)['Binar_OP130_Resultat_Global_v']\n",
       "\n",
       "# Filter variables with significant correlation\n",
       "significant_correlations = correlations[abs(correlations) > 0.1].index\n",
       "\n",
       "# Visualize relationships between significant variables and the target variable\n",
       "import seaborn as sns\n",
       "\n",
       "sns.pairplot(data=df, x_vars=significant_correlations, y_vars='Binar_OP130_Resultat_Global_v')\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "By explicitly setting `numeric_only=True`, you ensure that only numeric columns are included in the correlation calculation, which resolves the FutureWarning message.\n",
       "\n",
       "Please note that the correlation analysis and threshold values used in this code snippet are just examples. You may need to adjust them based on your specific dataset and requirements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "After running the code to explore correlations between the predictor variables and the target variable, I got this error:\n",
    "\"FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
    "  correlations = df.corr(method='spearman')['Binar_OP130_Resultat_Global_v']\"\n",
    "\n",
    "How do I resolve this?\n",
    "\"\"\"\n",
    "gpt_3_5.query_gpt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Here's a Python code snippet to visualize the outliers before dropping them:\n",
       "\n",
       "```python\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "\n",
       "# Identify outliers using Z-score\n",
       "from scipy.stats import zscore\n",
       "\n",
       "z_scores = zscore(df)\n",
       "outlier_indices = (abs(z_scores) > 3).any(axis=1)\n",
       "\n",
       "# Create a DataFrame with outlier indices and the target variable\n",
       "outliers_df = pd.DataFrame({'Outlier': outlier_indices, 'Target': df['Binar_OP130_Resultat_Global_v']})\n",
       "\n",
       "# Visualize outliers using box plots\n",
       "plt.figure(figsize=(10, 6))\n",
       "sns.boxplot(x='Outlier', y='Target', data=outliers_df)\n",
       "plt.xlabel('Outlier')\n",
       "plt.ylabel('Target')\n",
       "plt.title('Outliers visualization')\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "This code uses the Z-score method to identify outliers and creates a DataFrame `outliers_df` containing the outlier indices and the target variable. It then visualizes the outliers using box plots, where the x-axis represents the outliers and the y-axis represents the target variable.\n",
       "\n",
       "Running this code will help you visualize the outliers in relation to the target variable before deciding whether to drop them or apply any other outlier treatment techniques.\n",
       "\n",
       "Remember to adjust the threshold value in the `zscore()` function (`abs(z_scores) > 3`) based on your dataset and requirements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Instead of dropping the outliers after identifying them, I want to visualize them first.\n",
    "\n",
    "Write a python code to visualize the outliers before dropping them.\n",
    "\"\"\"\n",
    "gpt_3_5.query_gpt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The error you encountered occurs because the `zscore()` function from `scipy.stats` expects numeric values as input, but it encountered non-numeric columns in your dataframe. To fix this error, we need to exclude the non-numeric columns from the `zscore()` calculation.\n",
       "\n",
       "Here's an updated code snippet that handles this issue:\n",
       "\n",
       "```python\n",
       "import numpy as np\n",
       "\n",
       "# Identify numeric columns\n",
       "numeric_columns = df.select_dtypes(include=np.number).columns\n",
       "\n",
       "# Calculate z-scores for numeric columns only\n",
       "z_scores = zscore(df[numeric_columns])\n",
       "\n",
       "# Create a DataFrame with z-scores\n",
       "z_scores_df = pd.DataFrame(z_scores, columns=numeric_columns)\n",
       "\n",
       "# Identify outliers based on z-scores\n",
       "outlier_indices = (abs(z_scores_df) > 3).any(axis=1)\n",
       "\n",
       "# Visualize outliers using box plots\n",
       "plt.figure(figsize=(10, 6))\n",
       "sns.boxplot(x=outlier_indices, y=df['Binar_OP130_Resultat_Global_v'])\n",
       "plt.xlabel('Outlier')\n",
       "plt.ylabel('Target')\n",
       "plt.title('Outliers visualization')\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "In this updated code, we first identify the numeric columns in the dataframe using `select_dtypes()` from pandas. We then calculate the z-scores for only the numeric columns. The resulting z-scores are stored in a separate dataframe called `z_scores_df`.\n",
       "\n",
       "Next, we identify the outlier indices based on the z-scores using `(abs(z_scores_df) > 3).any(axis=1)`. Finally, we visualize the outliers using box plots, where the x-axis represents the outliers and the y-axis represents the target variable.\n",
       "\n",
       "By excluding non-numeric columns from the z-score calculation, you should be able to avoid the `TypeError` and visualize the outliers correctly.\n",
       "\n",
       "Remember to adjust the threshold value (`> 3`) in the outlier identification step based on your dataset and requirements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\" \n",
    "In running the code you suggested earlier, I got this error:\n",
    "\"TypeError: unsupported operand type(s) for /: 'str' and 'int'\"\n",
    "\n",
    "The error was traced back to this line of code:\n",
    "\"z_scores = zscore(df)\"\n",
    "\n",
    "Why am I getting this error and how do I fix it? \n",
    "\"\"\"\n",
    "\n",
    "gpt_3_5.query_gpt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Here's a Python code snippet to address the identified issues and prepare the data for the next steps:\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.impute import SimpleImputer\n",
       "from sklearn.preprocessing import StandardScaler\n",
       "\n",
       "# Load the dataset\n",
       "df = pd.read_csv('valeo_dataset.csv')\n",
       "\n",
       "# Drop features with over 50% missing values\n",
       "missing_values = df.isnull().sum() / len(df) * 100\n",
       "features_with_missing_values = missing_values[missing_values > 50].index\n",
       "df.drop(columns=features_with_missing_values, inplace=True)\n",
       "\n",
       "# Separate the target variable\n",
       "target = df['Binar_OP130_Resultat_Global_v']\n",
       "df.drop(columns='Binar_OP130_Resultat_Global_v', inplace=True)\n",
       "\n",
       "# Split the dataset into training and testing sets\n",
       "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.2, random_state=42)\n",
       "\n",
       "# Handle missing values with mean imputation\n",
       "imputer = SimpleImputer(strategy='mean')\n",
       "X_train_imputed = imputer.fit_transform(X_train)\n",
       "X_test_imputed = imputer.transform(X_test)\n",
       "\n",
       "# Scale the features\n",
       "scaler = StandardScaler()\n",
       "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
       "X_test_scaled = scaler.transform(X_test_imputed)\n",
       "```\n",
       "\n",
       "In this code snippet, we first load the dataset and drop the features with over 50% missing values.\n",
       "\n",
       "Next, we separate the target variable from the predictor variables.\n",
       "\n",
       "Then, we split the dataset into training and testing sets using `train_test_split()` from scikit-learn.\n",
       "\n",
       "After that, we handle missing values using mean imputation with `SimpleImputer()`. We fit the imputer on the training set and transform both the training and testing sets.\n",
       "\n",
       "Finally, we scale the features using `StandardScaler()` from scikit-learn. We fit the scaler on the training set (imputed and transformed) and apply the same scaling to the training and testing sets.\n",
       "\n",
       "Make sure to replace `'valeo_dataset.csv'` with the actual filename and path of the Valeo dataset on your local system.\n",
       "\n",
       "After running this code, you will have the prepared data ready for the next steps, such as model training and evaluation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\" \n",
    "So far, we have walked through the data understanding step and identified some issues with the data.\n",
    "\n",
    "Write a python code to address these issues and prepare the data for the next step.\n",
    "\"\"\"\n",
    "\n",
    "gpt_3_5.query_gpt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The error you encountered occurs because the `SimpleImputer` class from scikit-learn only works with numeric data and cannot handle non-numeric or categorical data by default.\n",
       "\n",
       "To fix this error, we need to handle the categorical variables separately before applying the imputation. One common approach is to use one-hot encoding to convert categorical variables into binary columns.\n",
       "\n",
       "Here's an updated code snippet that addresses this issue:\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.impute import SimpleImputer\n",
       "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
       "from sklearn.compose import ColumnTransformer\n",
       "\n",
       "# Load the dataset\n",
       "df = pd.read_csv('valeo_dataset.csv')\n",
       "\n",
       "# Drop features with over 50% missing values\n",
       "missing_values = df.isnull().sum() / len(df) * 100\n",
       "features_with_missing_values = missing_values[missing_values > 50].index\n",
       "df.drop(columns=features_with_missing_values, inplace=True)\n",
       "\n",
       "# Separate the target variable\n",
       "target = df['Binar_OP130_Resultat_Global_v']\n",
       "df.drop(columns='Binar_OP130_Resultat_Global_v', inplace=True)\n",
       "\n",
       "# Split the dataset into training and testing sets\n",
       "X_train, X_test, y_train, y_test = train_test_split"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\" \n",
    "In running the code for the data preparation steps, I got this error:\n",
    "\"Cannot use mean strategy with non-numeric data: could not convert string to float: 'I-B-XA1207672-190629-00771'\"\n",
    "\n",
    "The error was traced back to this line of code:\n",
    "\"X_train_imputed = imputer.fit_transform(X_train)\"\n",
    "\n",
    "Why am I getting this error and how do I fix it? \n",
    "\"\"\"\n",
    "\n",
    "gpt_3_5.query_gpt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens. However, your messages resulted in 4118 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39m \u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[39mYour response got truncated. Please respond to the prompt again\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m gpt_3_5\u001b[39m.\u001b[39;49mquery_gpt(prompt)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\OneDrive\\Desktop\\MSc. Bradford\\MSc. Dissertation\\open_ai\\gpt.py:16\u001b[0m, in \u001b[0;36mGPT.query_gpt\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog \u001b[39m=\u001b[39m [{\u001b[39m'\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mYou are an intelligent data science/analytics teaching assistant\u001b[39m\u001b[39m'\u001b[39m}]\n\u001b[0;32m     15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m:prompt})\n\u001b[1;32m---> 16\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion\u001b[39m.\u001b[39;49mcreate(model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog, temperature\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtemperature)\n\u001b[0;32m     17\u001b[0m answer \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m: answer})\n",
      "File \u001b[1;32mc:\\Users\\Owner\\anaconda3\\envs\\genai\\Lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\Owner\\anaconda3\\envs\\genai\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\anaconda3\\envs\\genai\\Lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\Owner\\anaconda3\\envs\\genai\\Lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    705\u001b[0m         ),\n\u001b[0;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Owner\\anaconda3\\envs\\genai\\Lib\\site-packages\\openai\\api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    765\u001b[0m     )\n\u001b[0;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 4118 tokens. Please reduce the length of the messages."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "prompt = \"\"\" \n",
    "Your response got truncated. Please respond to the prompt again\n",
    "\"\"\"\n",
    "\n",
    "gpt_3_5.query_gpt(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
